<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Transcription and GPT-4 Response</title>
</head>
<body>
    <h1>Audio Transcription and GPT-4 Response</h1>
    <button id="recordButton">Record</button>
    <p id="transcription">Transcribed text will appear here...</p>
    <p id="gptResponse">GPT-4 response will appear here...</p>
    <audio id="audioPlayback" controls style="display: none;"></audio>

    <script>

        let API_KEY = '...';

        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            alert('Your browser does not support audio recording.');
            throw new Error('Your browser does not support audio recording.');
        }

        const recordButton = document.getElementById('recordButton');
        const transcriptionParagraph = document.getElementById('transcription');
        const gptResponseParagraph = document.getElementById('gptResponse');

        let mediaRecorder;
        let audioChunks = [];
        let silenceTimeout;

        recordButton.addEventListener('click', () => {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
                recordButton.textContent = 'Record';
            } else {
                startRecording();
                recordButton.textContent = 'Stop';
            }
        });

        function startRecording() {
            navigator.mediaDevices.getUserMedia({ audio: true })
                .then(stream => {
                    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    const source = audioContext.createMediaStreamSource(stream);
                    const analyser = audioContext.createAnalyser();
                    source.connect(analyser);
                    analyser.fftSize = 2048;
                    const bufferLength = analyser.fftSize;
                    const dataArray = new Uint8Array(bufferLength);

                    mediaRecorder = new MediaRecorder(stream);
                    mediaRecorder.ondataavailable = event => {
                        audioChunks.push(event.data);
                    };
                    mediaRecorder.onstop = () => {
                        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                        audioChunks = [];
                        transcribeAudio(audioBlob);
                    };
                    mediaRecorder.start();

                    function checkSilence() {
                        analyser.getByteTimeDomainData(dataArray);
                        let silenceDetected = true;
                        for (let i = 0; i < bufferLength; i++) {
                            if (dataArray[i] > 128 + 2 || dataArray[i] < 128 - 2) {
                                silenceDetected = false;
                                break;
                            }
                        }
                        if (silenceDetected) {
                            if (!silenceTimeout) {
                                silenceTimeout = setTimeout(() => {
                                    if (mediaRecorder.state === 'recording') {
                                        mediaRecorder.stop();
                                        recordButton.textContent = 'Record';
                                    }
                                }, 2000);
                            }
                        } else {
                            if (silenceTimeout) {
                                clearTimeout(silenceTimeout);
                                silenceTimeout = null;
                            }
                        }
                        if (mediaRecorder.state === 'recording') {
                            requestAnimationFrame(checkSilence);
                        }
                    }

                    checkSilence();
                })
                .catch(error => {
                    console.error('Error starting recording', error);
                });
        }

        function transcribeAudio(audioBlob) {
            const formData = new FormData();
            formData.append('file', audioBlob);
            formData.append('model', 'whisper-1');

            fetch('https://api.openai.com/v1/audio/transcriptions', {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${API_KEY}`
                },
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                if (data.text) {
                    transcriptionParagraph.textContent = data.text;
                    sendToGPT(data.text);
                } else {
                    transcriptionParagraph.textContent = 'Transcription failed. Please try again.';
                }
            })
            .catch(error => {
                console.error('Error during transcription', error);
                transcriptionParagraph.textContent = 'Error during transcription. Please try again.';
            });
        }

        var messages = [
            {"role": "system", "content": "Du bist ein guter Freund."},
        ];

        function sendToGPT(transcribedText) {

            messages.push({"role": "user", "content": transcribedText});

            fetch('https://api.openai.com/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${API_KEY}`
            },
            body: JSON.stringify({
                model: 'gpt-4o', 
                messages: messages,
                max_tokens: 150
            })
        })
        .then(response => response.json())
        .then(data => {
            if (data.choices && data.choices.length > 0) {
                messages.push({"role": "assistant", "content": data.choices[0].message.content});
                const gptText = data.choices[0].message.content;
                gptResponseParagraph.textContent = gptText;
                generateSpeech(gptText);

            } else {
                gptResponseParagraph.textContent = 'GPT-4 response failed. Please try again.';
            }
        })
        .catch(error => {
            console.error('Error during GPT-4 request', error);
            gptResponseParagraph.textContent = 'Error during GPT-4 request. Please try again.';
        });
    }
    function generateSpeech(text) {
            fetch('https://api.openai.com/v1/audio/speech', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${API_KEY}`
                },
                body: JSON.stringify({
                    model: 'tts-1',
                    voice: 'onyx', // You can choose any available voice
                    input: text
                })
            })
            .then(response => response.blob())
            .then(audioBlob => {
                const audioUrl = URL.createObjectURL(audioBlob);
                audioPlayback.src = audioUrl;
                audioPlayback.play();
            })
            .catch(error => {
                console.error('Error during TTS request', error);
                alert('Error during TTS request. Please try again.');
            });
        }

        document.getElementById('audioPlayback').addEventListener('ended', function() {
            document.getElementById('recordButton').click();
        });
    </script>
</body>
</html>


