<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Transcription and GPT-4 Response</title>
</head>
<body>
    <h1>Audio Transcription and GPT-4 Response</h1>
    <button id="recordButton">Record</button>
    <p id="transcription">Transcribed text will appear here...</p>
    <p id="gptResponse">GPT-4 response will appear here...</p>
    <audio id="audioPlayback" controls style="display: none;"></audio>

    <script>

        let API_KEY = '...';
        

        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            alert('Your browser does not support audio recording.');
            throw new Error('Your browser does not support audio recording.');
        }

        const recordButton = document.getElementById('recordButton');
        const transcriptionParagraph = document.getElementById('transcription');
        const gptResponseParagraph = document.getElementById('gptResponse');

        let mediaRecorder;
        let audioChunks = [];

        recordButton.addEventListener('click', () => {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
                recordButton.textContent = 'Record';
            } else {
                startRecording();
                recordButton.textContent = 'Stop';
            }
        });

        function startRecording() {
            navigator.mediaDevices.getUserMedia({ audio: true })
                .then(stream => {
                    mediaRecorder = new MediaRecorder(stream);
                    mediaRecorder.ondataavailable = event => {
                        audioChunks.push(event.data);
                    };
                    mediaRecorder.onstop = () => {
                        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                        audioChunks = [];
                        transcribeAudio(audioBlob);
                    };
                    mediaRecorder.start();
                })
                .catch(error => {
                    console.error('Error accessing the microphone', error);
                    alert('Error accessing the microphone');
                });
        }

        function transcribeAudio(audioBlob) {
            const formData = new FormData();
            formData.append('file', audioBlob);
            formData.append('model', 'whisper-1');

            fetch('https://api.openai.com/v1/audio/transcriptions', {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${API_KEY}`
                },
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                if (data.text) {
                    transcriptionParagraph.textContent = data.text;
                    sendToGPT(data.text);
                } else {
                    transcriptionParagraph.textContent = 'Transcription failed. Please try again.';
                }
            })
            .catch(error => {
                console.error('Error during transcription', error);
                transcriptionParagraph.textContent = 'Error during transcription. Please try again.';
            });
        }

        var messages = [
            {"role": "system", "content": "Du bist ein guter Freund."},
        ];

        function sendToGPT(transcribedText) {

            messages.push({"role": "user", "content": transcribedText});

            fetch('https://api.openai.com/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${API_KEY}`
            },
            body: JSON.stringify({
                model: 'gpt-4o', 
                messages: messages,
                max_tokens: 150
            })
        })
        .then(response => response.json())
        .then(data => {
            if (data.choices && data.choices.length > 0) {
                messages.push({"role": "assistant", "content": data.choices[0].message.content});
                const gptText = data.choices[0].message.content;
                gptResponseParagraph.textContent = gptText;
                generateSpeech(gptText);

            } else {
                gptResponseParagraph.textContent = 'GPT-4 response failed. Please try again.';
            }
        })
        .catch(error => {
            console.error('Error during GPT-4 request', error);
            gptResponseParagraph.textContent = 'Error during GPT-4 request. Please try again.';
        });
    }
    function generateSpeech(text) {
            fetch('https://api.openai.com/v1/audio/speech', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${API_KEY}`
                },
                body: JSON.stringify({
                    model: 'tts-1',
                    voice: 'onyx', // You can choose any available voice
                    input: text
                })
            })
            .then(response => response.blob())
            .then(audioBlob => {
                const audioUrl = URL.createObjectURL(audioBlob);
                audioPlayback.src = audioUrl;
                audioPlayback.play();
            })
            .catch(error => {
                console.error('Error during TTS request', error);
                alert('Error during TTS request. Please try again.');
            });
        }

        document.getElementById('audioPlayback').addEventListener('ended', function() {
            document.getElementById('recordButton').click();
        });
    </script>
</body>
</html>


